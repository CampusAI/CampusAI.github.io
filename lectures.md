---
layout: page
title: Lectures
permalink: /lectures/
---

## Notes from Sergey Levine UC Berkeley CS-285

- [Lecture 1: Introduction](/lectures/lecture1)
    - Supervised Learning vs RL
    - Ways to learn
    - How to build intelligent machines
    - State of the art
- [Lecture 2: Imitation Learning](/lectures/lecture2)
    - Behavioral Cloning
    - Why doesn't it work?
<!-- - Lecture 3: TensorFlow Review -->
- [Lecture 4: Introduction to Reinforcement Learning](/lectures/lecture4)
    - Markov Decision Processes
    - The goal of RL
    - RL algorithms
- [Lecture 5: Policy Gradients](/lectures/lecture5)
    - Policy differentiation
    - The REINFORCE algorithm
    - Solving the causality issue
    - Baselines
    - Off-policy Policy Gradient
- [Lecture 6: Actor-Critic (AC) Algorithms](/lectures/lecture6)
    - Reducing Variance on Policy Gradients
    - Policy evaluation (MC vs Bootstrapping)
    - AC algorithm
- Lecture 7: Value Function Methods
- Lecture 8: Deep RL with Q-functions
- Lecture 9: Advanced Policy Gradients
- Lecture 10: Model-based Planning
- Lecture 11: Model-based Reinforcement Learning
- Lecture 12: Model-based Policy Learning
- Lecture 13: Variational Inference and Generative Models
- Lecture 14: Control as Inference
- Lecture 15: Inverse Reinforcement Learning
- Lecture 16: Transfer and Multi-task Learning
- Lecture 17: Distributed RL
- Lecture 18: Exploration (Part 1)
- Lecture 19: Exploration (Part 2)
- Lecture 20: Meta-learning
- Lecture 21: Information Theory, Open Problems

## Annex
- [Annex 1: MDP Basics](/lectures/basic_concepts)
- [Annex 2: Policy Expectations, Explained](/lectures/policy_expectations)
